(window.webpackJsonp=window.webpackJsonp||[]).push([[77],{331:function(t,e,o){"use strict";o.r(e);var a=o(0),_=Object(a.a)({},(function(){var t=this,e=t.$createElement,o=t._self._c||e;return o("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[o("h1",{attrs:{id:"모두를-위한-딥러닝-강좌-시즌-1-by-김성훈-교수"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#모두를-위한-딥러닝-강좌-시즌-1-by-김성훈-교수"}},[t._v("#")]),t._v(" 모두를 위한 딥러닝 강좌 시즌 1 by 김성훈 교수")]),t._v(" "),o("p",[t._v("홍콩과기대 김성훈 교수의 "),o("a",{attrs:{href:"https://www.youtube.com/watch?v=BS6O0zOGX4E&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm",target:"_blank",rel:"noopener noreferrer"}},[t._v("유투브 강의"),o("OutboundLink")],1),t._v('를 뒤늦게 몰아보고 있다. 생각보다 훨씬 초급자 수준에 맞추어져 있어서 진입장벽이 매우 낮다고 하겠다. 영상목록의 제목처럼, 정말 "모두를 위한" 강의가 아닌가 싶다.')]),t._v(" "),o("h2",{attrs:{id:"강의관련-요약"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#강의관련-요약"}},[t._v("#")]),t._v(" 강의관련 요약")]),t._v(" "),o("p",[t._v("초반부는 머신러닝에 관한 간단한 소개 정도로 요약할 수 있다. 알파고가 이세돌 기사를 이겼던 이후, 광풍이 불었던 (현재도 진행중인) 시기에 만들어진 강의여서 모두들 장미빛을 안고 강의를 듣고 있을 것 같다. 최근에 IBM 왓슨의 의료 프로젝트가 주춤하는 모양새인데, 너도나도 할 것없이 모여들었던 기술 투자에 대한 조정기가 아닐까 싶다.")]),t._v(" "),o("p",[t._v("이 강의는 lec과 lab 파트로 구분되는데, lec에서는 수식이나 개념을 설명하고, lab 파트에서는 구글 텐서플로우 코드로 구현한 것을 소개한다.")]),t._v(" "),o("p",[t._v("Lec 02에서는 선형회귀를 소개한다.")]),t._v(" "),o("p",[t._v("Lec 03에서는 cost 함수를 소개하고, cost 함수를 최소화함으로써 선형회귀 문제를 푼다는 걸 설명한다.")]),t._v(" "),o("p",[t._v("Lec 04에서는 행렬을 이용해서 여러가지 변수를 갖는 회귀식을 소개한다.")]),t._v(" "),o("p",[t._v("Lec 05에서는 Logistics 회귀를 소개한다. 데이터셋의 극치값이 추가됨에 따라 판별식이 달라지는 것을 방지한다. 시그모이드를 소개하고, 이 이후 Hypothesis가 선형식에서 로그회귀로 바뀐다. 로그회귀의 cost 함수는 로그형태를 취하는 형식으로 달라진다.")]),t._v(" "),o("p",[t._v("Lec 06에서는 softmax를 소개하는데, 시그모이드함수를 통해 0-1사이의 출력값을 가지게 되었고, 여러개의 결과(Y)에 대해서 값을 가지게 되는데, softmax를 통해 확률 값으로 변환한다. 선형회귀 모형의 cost 함수는 cross-entropy를 이용한다. 이전에 사용했던 로그회귀의 cost 함수가 사실 크로스 엔트로피 함수였다. D(S, L) = sum(L*los(S)) 로 쓴다. "),o("code",[t._v("one-hot")]),t._v("를 다루는 방법은 lab강의에서 다룬다. 분류를 할 때, 분류의 값이 "),o("code",[t._v("[1,2,3,4]")]),t._v("와 같이 되어 있을텐데, 이를 "),o("code",[t._v("[1 0 0 0], [0 1 0 0], [0 0 1 0]")]),t._v(" 등으로 바꾸어 주는것이다. 텐서플로우 코드에서는 랭크가 하나 늘기 때문에, "),o("code",[t._v("tf.one_hot")]),t._v(" 처리 후, "),o("code",[t._v("tf.reshape")]),t._v("를 해주어야 한다.")]),t._v(" "),o("p",[t._v("Lec 07에서는 학습 rate, 오버피팅, 일반화(Regulation)를 소개한다. 학습 rate는 0.01정도로 시작하는데, 발산여부와 수렴속도를 살펴보고 조정할 수 있다. 특성 변수가 큰 범위를 가지면, 학습이 잘 안되는 경우가 있다. 이럴 때, Nomalization이 필요하다. 그 중 하나가 Standardization인데, "),o("code",[t._v("x_std[:, 0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()")]),t._v("으로 쓸 수 있다. 정규분포라고 가정하면, 시그마 거리를 구하는 것이다. 오버피팅은 피쳐수를 줄인다던지, 데이터 자료수를 늘리거나, 일반화를 통해서 완화할 수 있다. cost 함수에 전체값의 합을 넣어주어 일반화하는것이다.")]),t._v(" "),o("p",[t._v("Lab 07에서 용어 정리도 하는데, "),o("code",[t._v("epoch")]),t._v("는 전체 학습데이터셋을 한번 다 돌리는 횟수, 유전 알고리즘의 세대와 비슷한 개념, "),o("code",[t._v("batch size")]),t._v("는 데이터셋이 커서 그 샘플링할 셋의 크기, "),o("code",[t._v("iterations")]),t._v("은 배치 사이즈에 따라 반복 횟수로 정의된다.")]),t._v(" "),o("p",[t._v("Lec 08에서는 딥러닝의 개념을 소개하고 있다. 초창기 인간의 뇌신경을 따라 만든 인공신경망은 XOR 문제를 풀 수 없었다. 마빈 민스키 교수가 MLP을 통해서 풀 수 있으나, 이를 학습할 수가 없다고 강변하였다. 1974년, 1982년 Paul Werbos, 1986년 Hinton에 의해서 backpropagation이 소개되었다. 또한 LeCun 교수가 컨볼루셔날 뉴럴 네트워크(CNN)을 소개하여 발전이 꽤 있었다. 문자인식 기술에 90% 이상의 인식율을 보였다고 한다. 하지만 1990년대 중반, 레이어가 많아지자 역전파가 제대로 작동하지 않았고, SVM, RandomForest 등이 간단하지만 더 나은 성능을 보여주면서 다시 역사의 그림자 속으로 들어가게 되었다.")]),t._v(" "),o("p",[t._v('캐나다의 CIFAR는 당장의 활용성이 떨어지더라도 펀딩을 지속적으로 하였는데, 역전파를 재발견한 Hinton 교수가 1987년 캐나다로 옮겨가면서 이 재단의 후원을 받았다. 그리고 2006년 Hinton교수의 "A fast learning algorithm for deep belief nets"와 2007년 Yoshua Bengio 교수의 "Greedy Layer-Wise Training of Deep Networks"라는 두 논문이 발표되었다. 2006년 논문은 초기 가중치에 따라 결과의 성능이 크게 달라짐을 밝혔고, 2007년 논문에서는 딥러닝을 통해 다양한 문제를 잘 풀어낼 수 있는 가능성을 보여주었다. 이때 딥러닝으로 리브랜딩이 되었다. 그 이후는 잘 알다시피, ImageNet에서 큰 성과를 보였고, 알파고, 음성인식, 유투브 자막 자동생성, 자율주행 등의 성과를 보이게 되었다.')]),t._v(" "),o("p",[t._v("Lab 08에서는 numpy의 인덱스 사용법을 설명하고 있는데, 일반적인 내용이므로 패스. rank는 괄호수를 세면 된다. Axis는 바깥괄호부터 0, 1, 2 이렇게 센다. 브로드캐스팅은 쉐입이 다를 경우, 자동으로 쉐입을 맞춰주는데, 잘 모르면 쉐입을 맞추어서 계산하자. 변수를 추가할 때는 stack() 메서드를 이용할 수 있다.")]),t._v(" "),o("p",[t._v("Lec 09에서는 XOR를 구현하기 위해서 NN을 설명한다. 반가산기처럼 뉴럴을 중첩해서 계산할 수 있으며, 초기값을 잘 정해주면 풀린다. 이 문제를 풀기 위해서 가중치를 잘 정해주어야 하는데, 민스키 교수가 지적했듯이 계산하기가 어렵다. 그래서 역전파 방법이 필요하다. forward propagation은 값을 넣어서 계산하면 되는데, back propagation은 편미분을 해서 구한다. 편미분값은 네트워크를 역순으로 따라오면서, 그 미분값을 계속 곱하면 되는데, 결국은 체인룰과 같은 형태가 되는것이다. 미분 역시 수치해석으로 풀이 가능하다. 미분을 설명하기 위해서 상당시간을 소요하는데, 공학전공자면 스킵할 수 있다.")])])}),[],!1,null,null,null);e.default=_.exports}}]);