(window.webpackJsonp=window.webpackJsonp||[]).push([[29],{318:function(t,a,r){"use strict";r.r(a);var s=r(0),e=Object(s.a)({},(function(){var t=this,a=t.$createElement,r=t._self._c||a;return r("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[r("h1",{attrs:{id:"전방-신경망의-학습"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#전방-신경망의-학습"}},[t._v("#")]),t._v(" 전방 신경망의 학습")]),t._v(" "),r("h2",{attrs:{id:"패스트푸드-문제"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#패스트푸드-문제"}},[t._v("#")]),t._v(" 패스트푸드 문제")]),t._v(" "),r("p",[t._v("햄버거, 감자튀김, 탄산음료로 구성된 메뉴에서 세트 메뉴의 가격을 추정하는 모형이 있다고 가정하자.")]),t._v(" "),r("p",[t._v("한가지 지능적인 해법으로 메뉴를 각각 하나씩 주문하여, 학습 효율을 높이는 방법이다. 매우 좋은 아이디어이나, 오로지 이 방법 만을 이용하여 풀 수 있는 현실문제가 거의 없다. 예를 들어, 이미지 인식과 같은 문제에서는 전혀 활용할 수 없는 방법이다.")]),t._v(" "),r("p",[t._v("여기서 오차함수를 도입하여, 예측값과 올바른 답 사이의 제곱합을 이용하여 오차를 구한다. 이와 같은 방식을 도입하는 이유는 연립방정식은 선형대수로 풀이가 가능하지만, 시그모이드, tanh, ReLU와 같이 비선형 뉴런을 사용하기 시작하면, 더 이상 선형대수적 해법을 사용할 수 없기 때문이다.")]),t._v(" "),r("h2",{attrs:{id:"경사-하강법"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#경사-하강법"}},[t._v("#")]),t._v(" 경사 하강법")]),t._v(" "),r("p",[t._v("파라미터들을 평면 축으로 두고, 오차를 z축으로 둔 그림을 상상하면, 사발모양의 그림을 생각할 수 있다. 이를 다시 파라미터 평면에서 살펴보면 등고선 형태로 도시할 수 있다. 등고선의 가장 깊은 곳이 최소오차가 될텐데, 최저점을 찾기 위해서는 가장 가파른 곳을 찾아서 내려가면 된다. 등고선에 대해 수직방향을 경사(gradient)라고 하고, 이는 해당 지점에서 가장 가파른 경사이다. 경사 하강법은 이러한 개념을 수학적으로 구현한 방법이다.")]),t._v(" "),r("h2",{attrs:{id:"델타-규칙과-학습률"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#델타-규칙과-학습률"}},[t._v("#")]),t._v(" 델타 규칙과 학습률")]),t._v(" "),r("p",[t._v("신경망에서는 가중치 파라미터 외에도 학습 과정을 수행하기 위해 학습 알고리즘에서 하이퍼파라미터를 이용하는데, 대표적인 것이 학습률이다.")]),t._v(" "),r("p",[t._v("학습률이 너무 크면, 수렴하지 못하고, 골짜기 양쪽을 건너뛰어다니게 된다. 반대로 학습률이 너무 작으면 하강 속도가 너무 느리게 된다.")]),t._v(" "),r("h2",{attrs:{id:"시그모이드-뉴런의-경사-하강법"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#시그모이드-뉴런의-경사-하강법"}},[t._v("#")]),t._v(" 시그모이드 뉴런의 경사 하강법")]),t._v(" "),r("p",[t._v("시그모이드 함수의 특성상 도함수가 매우 간단히 정리된다.")])])}),[],!1,null,null,null);a.default=e.exports}}]);